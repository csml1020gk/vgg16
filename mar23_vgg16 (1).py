# -*- coding: utf-8 -*-
"""mar23-vgg16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EUOiW_L85CfBBCk_e5xIGqttdIOtaLzM
"""

from google.colab import drive
drive.mount('/content/gdrive')
#https://deeplizard.com/learn/video/oDHpqu52soI

!cd /content/gdrive/MyDrive/kaggle

#!mkdir /content/gdrive/MyDrive/kaggle/data
#!unzip /content/gdrive/MyDrive/kaggle/aptos2019-blindness-detection.zip -d /content/gdrive/MyDrive/kaggle/data 
!ls /content/gdrive/MyDrive/kaggle/data/

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
print(os.listdir("/content/gdrive/MyDrive/kaggle/data"))
#https://www.kaggle.com/gkcsml1020/aptos-blindness-detection-basic-cnn/edit

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout, GaussianNoise, GaussianDropout
from keras.layers import Flatten, BatchNormalization
from keras.layers.convolutional import Conv2D, SeparableConv2D
from keras.constraints import maxnorm
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
from keras import backend as K
from keras import regularizers, optimizers
from keras.optimizers import Adam

train_df = pd.read_csv('/content/gdrive/MyDrive/kaggle/data/train.csv')
train_df['diagnosis'] = train_df['diagnosis'].astype('str')
train_df['id_code'] = train_df['id_code'].astype(str)+'.png'

from keras.preprocessing.image import ImageDataGenerator

datagen=ImageDataGenerator(
    rescale=1./255, 
    validation_split=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)
    #zca_whitening = True)

batch_size = 16
image_size = 224

train_image_path = '/content/gdrive/MyDrive/kaggle/data/train_images'
test_image_path = '/content/gdrive/MyDrive/kaggle/data/test_images'


train_gen=datagen.flow_from_dataframe(
    dataframe=train_df,
    directory=train_image_path,
    x_col="id_code",
    y_col="diagnosis",
    batch_size=batch_size,
    shuffle=True,
    class_mode="categorical",
    target_size=(image_size,image_size),
    subset='training')

test_gen=datagen.flow_from_dataframe(
    dataframe=train_df,
    directory=train_image_path,
    x_col="id_code",
    y_col="diagnosis",
    batch_size=batch_size,
    shuffle=True,
    class_mode="categorical", 
    target_size=(image_size,image_size),
    subset='validation')

train_df['id_code'].sample(1)

import tensorflow as tf
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input
import numpy as np
vgg16_model = tf.keras.applications.vgg16.VGG16()

vgg16_model.summary()

model = Sequential()
for layer in vgg16_model.layers[:-1]:
    model.add(layer)

for layer in model.layers:
    layer.trainable = False

model.add(Dense(units=5, activation='softmax'))

model.summary()

model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

history_vgg16 = model.fit(x=train_gen,
          steps_per_epoch=len(train_gen),
          validation_data=test_gen,
          validation_steps=len(test_gen),
          epochs=3,
          verbose=2
)

hist_df = pd.DataFrame(history_vgg16.history) 

hist_csv_file = '/content/gdrive/MyDrive/kaggle/results/history-vgg16-mar23.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

history_df = pd.read_csv('/content/gdrive/MyDrive/kaggle/results/history-vgg16-mar23.csv')

#plot learning curve 
#https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/
#Reviewing learning curves of models during training can be used to diagnose problems with learning, such as an underfit or overfit model,
# as well as whether the training and validation datasets are suitably representative.
import matplotlib.pyplot as plt
plt.plot(history_df['accuracy'])
plt.plot(history_df['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig('/content/gdrive/MyDrive/kaggle/data/graph-modelvgg16-mar23-1acc')
plt.show()
# summarize history for loss
plt.plot(history_df['loss'])
plt.plot(history_df['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig('/content/gdrive/MyDrive/kaggle/data/graph-modelvgg16-mar23-1loss')
plt.show()

"""Learning curves are plots that show changes in learning performance over time in terms of experience.
Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.
Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain.

Underfit : the model is able to obtain a sufficiently low error  value on the training dataset and has scope for improvement. More epochs can give better answers.


Overfit: measured by performance of the model on the validation set . The plot of training loss decreases over time and the validation loss decreases to a certain point and begins increasing again. The inflection point in the validation plot could be the point where training could be halted.
Again, more epoch can help give better answers.

Good fit : The training and validation loss  does not decrease to a point of stability with a minimal gap between the training and validation plots.
More epochs can help determine good fit. The loss on the training set is less than the loss on the validation set. 

The training and validation datasets were generated from the train dataset and all images are representative examples. We can rule out unrepresentative datasets. 

The conclusion is that we need to train the data on more epochs.
We can
1. consider 5 or 10 epochs 
2. use early stopping on the loss metric and mode checkpoint to save best weights



"""

from keras.preprocessing.image import load_img
# load an image from file
image = load_img('/content/gdrive/MyDrive/kaggle/data/train_images/0abf0c485f66.png', target_size=(224, 224))

submission_df = pd.read_csv('/content/gdrive/MyDrive/kaggle/data/sample_submission.csv')
#submission_df['diagnosis'] = submission_df['diagnosis'].astype('str')
submission_df['id_code'] = submission_df['id_code'].astype(str)+'.png'
#test dataset 


sub_image_path = '/content/gdrive/MyDrive/kaggle/data/test_images'
submission_datagen=ImageDataGenerator(rescale=1./255)
submission_gen=submission_datagen.flow_from_dataframe(
    dataframe=submission_df,
    directory=sub_image_path,
    x_col="id_code",    
    batch_size=batch_size,
    shuffle=True,
    class_mode=None,
    target_size=(image_size,image_size))

# predict the probability across all output classes
STEP_SIZE_TEST=submission_gen.n//submission_gen.batch_size
submission_gen.reset()
pred=model.predict_generator(submission_gen,
steps=STEP_SIZE_TEST,
verbose=1)

predicted_class_indices=np.argmax(pred,axis=1)

predicted_class_indices.size

labels = (train_gen.class_indices)
labels = dict((v,k) for k,v in labels.items())
labels
predictions = [labels[k] for k in predicted_class_indices]

len(predictions)

#model predicts every image as a 1 

from numpy import array 
filenames = submission_gen.filenames
filenames[0]
print(type(filenames))
filenames = array(filenames)
print(type(filenames))

filenames = filenames[0:1920]

results = pd.DataFrame({'id_code':filenames, 'diagnosis':predictions})
results['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])
results.to_csv('/content/gdrive/MyDrive/kaggle/submission-vgg16.csv',index=False)
results.head(10)

from google.colab import auth
auth.authenticate_user()

CLOUD_PROJECT = 'vgg16mar23'
BUCKET = 'gs://' + CLOUD_PROJECT

!gcloud config set project $CLOUD_PROJECT

!gsutil mb $BUCKET
print(BUCKET)
model.save(BUCKET, save_format='tf')
MODEL = 'vgg16mar23'
!gcloud ai-platform models create $MODEL --regions=us-central1

!echo "# vgg16" >> README.md
!git init
!git add README.md
!git commit -m "first commit"
!git branch -M main
!git remote add origin https://github.com/csml1020gk/vgg16.git
!git push -u origin main

!git config --global user.email "csml1020gk@gmail.com"
!git config --global user.name "caml1020gk"



/content/gdrive/MyDrive/kaggle/data/

"""We have successfully fine tuned a pretrained vgg16 model to predict the class of images

1) Use Google Colab to build, train and export a Tensorflow model.

2) Upload the model to a GCP storage bucket.

3) Load the custom model into Googleâ€™s AI Platform and prepare it for serving.

4) Deploy a Python web app on App Engine to interface with AI-Platform and run inference on our hosted model.

5) Send data to our front end application and markup the predictions.

Save model as h5 
Deploy to GCP 

https://www.tensorflow.org/guide/saved_model

https://cloud.google.com/ai-platform/prediction/docs/deploying-models

https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction
"""

https://colab.research.google.com/github/GoogleCloudPlatform/cloudml-samples/blob/master/notebooks/tensorflow/getting-started-keras.ipynb#scrollTo=i2qsxysTVc-l

PROJECT_ID = "vgg16-model" #@param {type:"string"}
! gcloud config set project $PROJECT_ID

project_id = "csml1030-prep"
bucket_name = "dr-model"
!gcloud config set project {project_id}

